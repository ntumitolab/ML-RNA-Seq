# -*- coding: utf-8 -*-
"""RNA Seq of GSE152075 for random select 40 genes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l37lpzb4nmQDH9qvM4zZ3kHSXbfPR5LV
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install shap
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn.metrics as metrics
import xgboost
import shap
import joblib
import math

from numpy import mean, std
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.base import clone
from sklearn.linear_model import LogisticRegression
from sklearn.utils import shuffle
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, make_scorer, roc_curve, auc
from sklearn.feature_selection import SelectKBest, chi2, RFE
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold,cross_val_predict, learning_curve
from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN
from imblearn.under_sampling import TomekLinks
from sklearn.pipeline import Pipeline, make_pipeline

# %matplotlib inline

! python --version

print("shap version:", shap.__version__)

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/GSE152075_raw_counts_GEO _correct to CSV_for_COVID19 for Python ML.csv',dtype=object)
df.head()

# Transpose
data = df.values
index1 = list(df.keys())
data = list(map(list, zip(*data)))
data = pd.DataFrame(data, index=index1)
data.to_csv('TML.csv', header=0)

df = pd.read_csv('TML.csv',dtype=object)
df.head()

df = df.drop(df.columns[0], axis=1)
df.head()

df['target'] = df.target
df.target.value_counts()

# all genes and random selected genes
df_removetarget = df.iloc[:, :-1]
df_removetarget = df_removetarget.sample(n = 40, random_state = 28, axis=1)
print(df_removetarget)
df_removetarget.head()

# GSE152075
# X = df_removetarget_reduced
X = df_removetarget
y = df.iloc[:, -1]
X_number = X.values
y_index = y.values

# standardize the dataset
scaler = MinMaxScaler()
X_number = scaler.fit_transform(X_number)
y_index = y_index.astype(int)

# Do not resample the test and index set
# y_index = smote.fit_resample(y_index)

print(X_number.shape)
print(y_index.shape)
print(X_number)
print(y_index)
print("In y_index:")
print("Number of 0s:", len(y_index) - np.count_nonzero(y_index))
print("Number of 1s:", np.count_nonzero(y_index))

# Cross Validation performance and AUC of machine learning for 4 models in GSE152075
models = {
    "XGBoost": XGBClassifier(colsample_bytree=0.9, learning_rate=0.1, max_depth=10, n_estimators=50, random_state=42),
    "RandomForestClassifier": RandomForestClassifier(max_depth=10, min_samples_split=5, n_estimators=100, random_state=42),
    "LogisticRegression": LogisticRegression(C=50, max_iter=5000, random_state=42),
    "SVC": SVC(kernel='rbf', C=100, gamma=0.01, probability=True, random_state=42)
}

y_index = y_index.astype(int)  # Ensure y_index is of type int

def confusion_metrics_and_report(conf_matrix):
    # Extract metrics from the confusion matrix
    TP = conf_matrix[1][1]
    TN = conf_matrix[0][0]
    FP = conf_matrix[0][1]
    FN = conf_matrix[1][0]

    print('True Positives:', TP)
    print('True Negatives:', TN)
    print('False Positives:', FP)
    print('False Negatives:', FN)

    # Calculate metrics
    accuracy = (TP + TN) / (TP + TN + FP + FN)
    mis_classification = 1 - accuracy
    sensitivity = TP / (TP + FN)
    specificity = TN / (TN + FP)
    precision = TP / (TP + FP)
    f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)

    conf_accuracy = (float(TP+TN) / float(TP + TN + FP + FN))
    conf_misclassification = 1 - conf_accuracy
    conf_sensitivity = (TP / float(TP + FN))
    conf_specificity = (TN / float(TN + FP))
    conf_precision = (TP / float(TP + FP))
    conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))

   # Print metrics
    print('-'*50)
    print(f'Accuracy: {round(conf_accuracy,3)}')
    print(f'Mis-Classification: {round(conf_misclassification,3)}')
    print(f'Sensitivity: {round(conf_sensitivity,3)}')
    print(f'Specificity: {round(conf_specificity,3)}')
    print(f'Precision: {round(conf_precision,3)}')
    print(f'f_1 Score: {round(conf_f1,3)}')

   # Matthews Correlation Coefficient
    MCC_numerator = (TP * TN) - (FP * FN)
    MCC_denominator = math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))

   # Prevent 0
    if MCC_denominator == 0:
        MCC = 0
    else:
        MCC = MCC_numerator / MCC_denominator
    print(f'Matthews Correlation Coefficient: {round(MCC, 3)}')

    # plot confusion_metrics
    cm_df = pd.DataFrame(conf_matrix, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])
    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g')
    plt.title('Confusion Matrix')
    plt.show()

def evaluate_model(name, model, X, y, mean_fpr, tprs, aucs):
    print(f"Evaluating model: {name}")
    print('-'*50)

    smote = SMOTE(random_state=42)
    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

    all_preds = np.zeros(y.shape)

    for train_index, test_index in skf.split(X, y):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # Oversample the training data
        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

        model_clone = clone(model)
        model_clone.fit(X_train_resampled, y_train_resampled)

        # Store predictions for computing the confusion matrix later
        all_preds[test_index] = model_clone.predict(X_test)

        # Compute ROC curve and AUC
        probas_ = model_clone.predict_proba(X_test)
        fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])

        tprs.append(np.interp(mean_fpr, fpr, tpr))
        tprs[-1][0] = 0.0
        roc_auc = auc(fpr, tpr)
        aucs.append(roc_auc)

    # Compute the confusion matrix and related metrics
    conf_matrix = confusion_matrix(y, all_preds)
    confusion_metrics_and_report(conf_matrix)
    print("\n\n")

mean_fpr = np.linspace(0, 1, 100)
tprs = []
aucs = []

for name, model in models.items():
    evaluate_model(name, model, X_number, y_index, mean_fpr, tprs, aucs)

# Plot all the ROC Curves on a single plot
plt.figure(figsize=(10, 8))
plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=0.8)
for i, name in enumerate(models.keys()):
    mean_tpr = np.mean(tprs[i::len(models)], axis=0)
    mean_tpr[-1] = 1.0
    mean_auc = auc(mean_fpr, mean_tpr)
    plt.plot(mean_fpr, mean_tpr, label=f'{name} (AUC = {mean_auc:.3f})')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for All Models')
plt.legend(loc='lower right')
plt.show()


# Train each model and save it
for model_name, model in models.items():
    model.fit(X_number, y_index)
    # Save the trained model
    joblib.dump(model, f"{model_name}.pkl")