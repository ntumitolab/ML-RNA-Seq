# -*- coding: utf-8 -*-
"""RNA Seq of GSE152075, GSE163151, GSE157103, GSE152641 for 40 sig genes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k-F4p3sEIizF3-Eq5ZRIm3_w8p4lCwsO
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install shap
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn.metrics as metrics
import xgboost
import shap
import joblib

from numpy import mean, std
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.base import clone
from sklearn.linear_model import LogisticRegression
from sklearn.utils import shuffle
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, make_scorer, roc_curve, auc
from sklearn.feature_selection import SelectKBest, chi2, RFE
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold,cross_val_predict, learning_curve
from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN
from imblearn.under_sampling import TomekLinks
from sklearn.pipeline import Pipeline, make_pipeline

# %matplotlib inline

! python --version

print("shap version:", shap.__version__)

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/GSE152075_raw_counts_GEO _correct to CSV_for_COVID19 for Python ML.csv',dtype=object)
df.head()

# Transpose
data = df.values
index1 = list(df.keys())
data = list(map(list, zip(*data)))
data = pd.DataFrame(data, index=index1)
data.to_csv('TML.csv', header=0)

df = pd.read_csv('TML.csv',dtype=object)
df.head()

df = df.drop(df.columns[0], axis=1)
df.head()

df['target'] = df.target
df.target.value_counts()

# Top 40 significantly expressed genes in GSE152075
df_removetarget = df.iloc[:, :-1]
df_removetarget = df_removetarget[['IFI44L', 'XAF1', 'IFIT1', 'OAS3', 'OAS2', 'IFIT3', 'IFIT2', 'RSAD2', 'IGFBP2', 'DDX58',\
         'GBP1', 'TRIM22', 'EPSTI1', 'MX2', 'CD163', 'CMPK2', 'HERC6', 'SAMD9', 'CXCL10', 'GBP4',\
         'CRIP1', 'PARP9', 'RPLP1', 'DDX60', 'IFI44', 'IFIT5', 'RPS21', 'RPS8', 'FPR3', 'PCSK5',\
         'SAMD9L', 'DDX60L', 'OASL', 'RPL13A', 'CD300E', 'PLA2G7', 'ZEB2', 'SBK1', 'PRDX5', 'RRAD']]
print(df_removetarget)
df_removetarget.head()
#df_removetarget.to_csv('sig without target.csv')

X = df_removetarget
y = df.iloc[:, -1]
print(X)
print(y)

# GSE152075
X = df_removetarget
y = df.iloc[:, -1]
X_number = X.values
y_index = y.values

# standardize the dataset
scaler = MinMaxScaler()
X_number = scaler.fit_transform(X_number)
y_index = y_index.astype(int)

# Do not resample the test and index set
# y_index = smote.fit_resample(y_index)

print(X_number.shape)
print(y_index.shape)
print(X_number)
print(y_index)
print("In y_index:")
print("Number of 0s:", len(y_index) - np.count_nonzero(y_index))
print("Number of 1s:", np.count_nonzero(y_index))

plt.figure( figsize=(10,5) )
y_train_series = pd.Series(y_index)
y_train_series.value_counts().plot( kind='pie', colors=['lightcoral','skyblue'], autopct='%1.2f%%' )
plt.title( 'Pos/Neg' )
plt.ylabel( '' )
plt.show()

# Cross Validation performance and AUC of machine learning for 4 models in GSE152075
models = {
    "XGBoost": XGBClassifier(colsample_bytree=0.9, learning_rate=0.1, max_depth=10, n_estimators=50, random_state=42),
    "RandomForestClassifier": RandomForestClassifier(max_depth=10, min_samples_split=5, n_estimators=100, random_state=42),
    "LogisticRegression": LogisticRegression(C=50, max_iter=5000, random_state=42),
    "SVC": SVC(kernel='rbf', C=100, gamma=0.01, probability=True, random_state=42)
}

y_index = y_index.astype(int)  # Ensure y_index is of type int

def confusion_metrics_and_report(conf_matrix):
    # Extract metrics from the confusion matrix
    TP = conf_matrix[1][1]
    TN = conf_matrix[0][0]
    FP = conf_matrix[0][1]
    FN = conf_matrix[1][0]

    print('True Positives:', TP)
    print('True Negatives:', TN)
    print('False Positives:', FP)
    print('False Negatives:', FN)

    # Calculate metrics
    accuracy = (TP + TN) / (TP + TN + FP + FN)
    mis_classification = 1 - accuracy
    sensitivity = TP / (TP + FN)
    specificity = TN / (TN + FP)
    precision = TP / (TP + FP)
    f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)

    conf_accuracy = (float(TP+TN) / float(TP + TN + FP + FN))
    conf_misclassification = 1 - conf_accuracy
    conf_sensitivity = (TP / float(TP + FN))
    conf_specificity = (TN / float(TN + FP))
    conf_precision = (TP / float(TP + FP))
    conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))

   # Print metrics
    print('-'*50)
    print(f'Accuracy: {round(conf_accuracy,3)}')
    print(f'Mis-Classification: {round(conf_misclassification,3)}')
    print(f'Sensitivity: {round(conf_sensitivity,3)}')
    print(f'Specificity: {round(conf_specificity,3)}')
    print(f'Precision: {round(conf_precision,3)}')
    print(f'f_1 Score: {round(conf_f1,3)}')

    # plot confusion_metrics
    cm_df = pd.DataFrame(conf_matrix, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])
    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g')
    plt.title('Confusion Matrix')
    plt.show()

def evaluate_model(name, model, X, y, mean_fpr, tprs, aucs):
    print(f"Evaluating model: {name}")
    print('-'*50)

    smote = SMOTE(random_state=42)
    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

    all_preds = np.zeros(y.shape)

    for train_index, test_index in skf.split(X, y):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # Oversample the training data
        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

        model_clone = clone(model)
        model_clone.fit(X_train_resampled, y_train_resampled)

        # Store predictions for computing the confusion matrix later
        all_preds[test_index] = model_clone.predict(X_test)

        # Compute ROC curve and AUC
        probas_ = model_clone.predict_proba(X_test)
        fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])

        tprs.append(np.interp(mean_fpr, fpr, tpr))
        tprs[-1][0] = 0.0
        roc_auc = auc(fpr, tpr)
        aucs.append(roc_auc)

    # Compute the confusion matrix and related metrics
    conf_matrix = confusion_matrix(y, all_preds)
    confusion_metrics_and_report(conf_matrix)
    print("\n\n")

mean_fpr = np.linspace(0, 1, 100)
tprs = []
aucs = []

for name, model in models.items():
    evaluate_model(name, model, X_number, y_index, mean_fpr, tprs, aucs)

# Plot all the ROC Curves on a single plot
plt.figure(figsize=(10, 8))
plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=0.8)
for i, name in enumerate(models.keys()):
    mean_tpr = np.mean(tprs[i::len(models)], axis=0)
    mean_tpr[-1] = 1.0
    mean_auc = auc(mean_fpr, mean_tpr)
    plt.plot(mean_fpr, mean_tpr, label=f'{name} (AUC = {mean_auc:.3f})')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for All Models')
plt.legend(loc='lower right')
plt.show()


# Train each model and save it
for model_name, model in models.items():
    model.fit(X_number, y_index)
    # Save the trained model
    joblib.dump(model, f"{model_name}.pkl")

# SMOTE seperately in test sets - Learning curve for 4 models results in GSE152075
def plot_learning_curve_with_smote(estimator, title, X, y, cv=None, scoring=None):
    fig, axes = plt.subplots(1, 1, figsize=(5, 5))

    axes.set_title(title)
    axes.set_xlabel("Training examples")
    axes.set_ylabel(scoring)

    train_sizes = np.linspace(.1, 1.0, 5)
    smote = SMOTE(random_state=42)

    all_train_scores = []
    all_test_scores = []

    for frac in train_sizes:
        current_train_scores = []
        current_test_scores = []
        for train_index, test_index in cv.split(X, y):
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]

            # Resample only a fraction of the training data
            subset_idx = np.random.choice(len(X_train), int(frac * len(X_train)), replace=False)
            X_train_subset = X_train[subset_idx]
            y_train_subset = y_train[subset_idx]

            # Apply SMOTE
            X_train_resampled, y_train_resampled = smote.fit_resample(X_train_subset, y_train_subset)

            estimator_clone = clone(estimator)
            estimator_clone.fit(X_train_resampled, y_train_resampled)

            current_train_scores.append(accuracy_score(y_train_resampled, estimator_clone.predict(X_train_resampled)))
            current_test_scores.append(accuracy_score(y_test, estimator_clone.predict(X_test)))

        all_train_scores.append(current_train_scores)
        all_test_scores.append(current_test_scores)

    train_scores_mean = np.mean(all_train_scores, axis=1)
    train_scores_std = np.std(all_train_scores, axis=1)
    test_scores_mean = np.mean(all_test_scores, axis=1)
    test_scores_std = np.std(all_test_scores, axis=1)

    # Plotting the learning curves
    axes.grid()
    axes.fill_between(train_sizes*len(X), train_scores_mean - train_scores_std,
                      train_scores_mean + train_scores_std, alpha=0.1, color="r")
    axes.fill_between(train_sizes*len(X), test_scores_mean - test_scores_std,
                      test_scores_mean + test_scores_std, alpha=0.1, color="g")
    axes.plot(train_sizes*len(X), train_scores_mean, 'o-', color="r", label="Training score")
    axes.plot(train_sizes*len(X), test_scores_mean, 'o-', color="g", label="Cross-validation score")
    axes.legend(loc="best")
    plt.tight_layout()

    return plt



X, y = shuffle(X_number, y_index, random_state=42)
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

for name, model in models.items():
    title = f"Learning Curves with SMOTE ({name})"
    plot_learning_curve_with_smote(model, title, X_resampled, y_resampled, cv=StratifiedKFold(10), scoring="accuracy")
    plt.show()

# SMOTE seperately in test sets - check over or under fitting by cross validation with test set in GSE152075

def check_over_under_fitting_with_smote_and_testset(name, model, X, y):
    print(f"Evaluating model: {name}")
    print('-'*50)

    # Splitting the data into training/validation and test sets
    X_train_val, X_test_final, y_train_val, y_test_final = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    train_scores = []
    val_scores = []
    smote = SMOTE(random_state=42)

    for train_index, test_index in skf.split(X_train_val, y_train_val):
        X_train, X_val = X_train_val[train_index], X_train_val[test_index]
        y_train, y_val = y_train_val[train_index], y_train_val[test_index]

        # Apply SMOTE only on the training data
        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

        model_clone = clone(model)
        model_clone.fit(X_train_resampled, y_train_resampled)

        train_scores.append(model_clone.score(X_train_resampled, y_train_resampled))
        val_scores.append(model_clone.score(X_val, y_val))

    mean_train_score = np.mean(train_scores)
    mean_val_score = np.mean(val_scores)

    print(f"Mean Training Score: {mean_train_score:.3f}")
    print(f"Mean Validation Score: {mean_val_score:.3f}")

    # Check the performance on the final test set
    test_score = model_clone.score(X_test_final, y_test_final)
    print(f"Final Test Score: {test_score:.3f}")

    # Check for overfitting or underfitting
    if mean_train_score > mean_val_score + 0.1:
        print("Model might be overfitting.")
    elif mean_train_score < 0.7:
        print("Model might be underfitting.")
    else:
        print("Model seems to be fitting appropriately.")
    print("\n\n")

for name, model in models.items():
    check_over_under_fitting_with_smote_and_testset(name, model, X_number, y_index)

# SMOTE seperately in test sets - check over or under fitting by train_test_split with test set in GSE152075

def check_overfitting_with_train_test_split(name, model, X, y):
    print(f"Evaluating model: {name}")
    print('-'*50)

    # Split the data into training, validation, and test sets. devided by 70-15-15
    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)
    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp)  # 0.1765 is approximately 15/85

    # Apply SMOTE on the training data
    smote = SMOTE(random_state=42)
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

    # Fit the model
    model.fit(X_train_resampled, y_train_resampled)

    # Get the scores for training, validation, and test sets
    train_score = accuracy_score(y_train_resampled, model.predict(X_train_resampled))
    val_score = accuracy_score(y_val, model.predict(X_val))
    test_score = accuracy_score(y_test, model.predict(X_test))

    print(f"Training Accuracy: {train_score:.3f}")
    print(f"Validation Accuracy: {val_score:.3f}")
    print(f"Test Accuracy: {test_score:.3f}")

    # Check for overfitting or underfitting based on training and validation accuracies
    if train_score > val_score + 0.1:
        print(f"{name} might be overfitting.")
    elif train_score < 0.7:
        print(f"{name} might be underfitting.")
    else:
        print(f"{name} seems to be fitting appropriately.")
    print("-"*50)


for name, model in models.items():
    check_overfitting_with_train_test_split(name, clone(model), X_number, y_index)

# without SMOTE seperately in test sets - Cross Validation for 4 models results in GSE152075
# models = {
#     "XGBoost": XGBClassifier(colsample_bytree=0.9, learning_rate=0.1, max_depth=10, n_estimators=50, random_state=42),
#     "RandomForestClassifier": RandomForestClassifier(max_depth=10, min_samples_split=5, n_estimators=100, random_state=42),
#     "LogisticRegression": LogisticRegression(C=50, max_iter=5000, random_state=42),
#     "SVC": SVC(kernel='rbf', C=100, gamma=0.01, probability=True, random_state=42)
# }

# y_index = y_index.astype(int)  # Ensure y_index is of type int

# def confusion_metrics_and_report(conf_matrix):
#     # Extract metrics from the confusion matrix
#     TP = conf_matrix[1][1]
#     TN = conf_matrix[0][0]
#     FP = conf_matrix[0][1]
#     FN = conf_matrix[1][0]

#     print('True Positives:', TP)
#     print('True Negatives:', TN)
#     print('False Positives:', FP)
#     print('False Negatives:', FN)

#     # Calculate metrics
#     accuracy = (TP + TN) / (TP + TN + FP + FN)
#     mis_classification = 1 - accuracy
#     sensitivity = TP / (TP + FN)
#     specificity = TN / (TN + FP)
#     precision = TP / (TP + FP)
#     f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)

#     conf_accuracy = (float(TP+TN) / float(TP + TN + FP + FN))
#     conf_misclassification = 1 - conf_accuracy
#     conf_sensitivity = (TP / float(TP + FN))
#     conf_specificity = (TN / float(TN + FP))
#     conf_precision = (TP / float(TP + FP))
#     conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))

#    # Print metrics
#     print('-'*50)
#     print(f'Accuracy: {round(conf_accuracy,3)}')
#     print(f'Mis-Classification: {round(conf_misclassification,3)}')
#     print(f'Sensitivity: {round(conf_sensitivity,3)}')
#     print(f'Specificity: {round(conf_specificity,3)}')
#     print(f'Precision: {round(conf_precision,3)}')
#     print(f'f_1 Score: {round(conf_f1,3)}')

#     # plot confusion_metrics
#     cm_df = pd.DataFrame(conf_matrix, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])
#     sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g')
#     plt.title('Confusion Matrix')
#     plt.show()

# def evaluate_model(name, model, X, y):
#     print(f"Evaluating model: {name}")
#     print('-'*50)

#     # Step 1: Perform cross-validation and get accuracy
#     cv_scores = cross_val_score(model, X, y, cv=10, scoring='accuracy')
#     print("Cross-validated Accuracy: %.3f +/- %.3f" % (cv_scores.mean(), cv_scores.std()))

#     # Step 2: Get predictions from cross-validation
#     cv_preds = cross_val_predict(model, X, y, cv=10)

#     # Step 3: Compute the confusion matrix and related metrics
#     conf_matrix = confusion_matrix(y, cv_preds)
#     confusion_metrics_and_report(conf_matrix)
#     print("\n\n")

# for name, model in models.items():
#     evaluate_model(name, model, X_number, y_index)


# # learning_curve to see if overfit
# def plot_learning_curve(model, X, y, title):
#     train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=10, scoring="accuracy", n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 5))

#     train_mean = np.mean(train_scores, axis=1)
#     train_std = np.std(train_scores, axis=1)
#     test_mean = np.mean(test_scores, axis=1)
#     test_std = np.std(test_scores, axis=1)

#     plt.plot(train_sizes, train_mean, '--', color="#111111",  label="Training score")
#     plt.plot(train_sizes, test_mean, color="#111111", label="Cross-validation score")

#     plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color="#DDDDDD")
#     plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color="#DDDDDD")

#     plt.title(title)
#     plt.xlabel("Training Set Size"), plt.ylabel("Accuracy Score"), plt.legend(loc="best")
#     plt.tight_layout()
#     plt.show()

# # Plot the learning curve for each model
# for name, model in models.items():
#     plot_learning_curve(model, X_number, y_index, title=f"Learning Curve for {name}")


# # Train each model and save it
# for model_name, model in models.items():
#     model.fit(X_number, y_index)
#     # Save the trained model
#     joblib.dump(model, f"{model_name}.pkl")

# # without SMOTE seperately in test sets - AUC for 4 models results in GSE152075

# # X_numbera = np.array(X_number)
# # y_indexa = np.array(y_index)

# # Setting up plot for ROC Curves
# plt.figure(figsize=(12, 8))
# plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=0.8)

# cv = StratifiedKFold(n_splits=10)

# for name, model in models.items():
#     tprs = []
#     aucs = []
#     mean_fpr = np.linspace(0, 1, 100)

#     for train, test in cv.split(X_number, y_index):
#         model.fit(X_number[train], y_index[train])
#         probs = model.predict_proba(X_number[test])
#         predsroc = probs[:, 1]
#         fpr, tpr, _ = roc_curve(y_index[test], predsroc)
#         tprs.append(np.interp(mean_fpr, fpr, tpr))
#         tprs[-1][0] = 0.0
#         roc_auc = auc(fpr, tpr)
#         aucs.append(roc_auc)

#     mean_tpr = np.mean(tprs, axis=0)
#     mean_tpr[-1] = 1.0
#     mean_auc = np.mean(aucs)
#     plt.plot(mean_fpr, mean_tpr, label='%s (AUC = %0.3f)' % (name, mean_auc), lw=2, alpha=0.8)

# plt.xlabel('False Positive Rate')
# plt.ylabel('True Positive Rate')
# plt.title('ROC Curves of Models')
# plt.legend(loc="lower right")
# plt.show()

df151 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/GSE163151 for COVID and Donor without virus for Python ML.csv',dtype=object)
df151.head()

df103 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/GSE157103_genes.ec_raw counts_for Python ML.csv',dtype=object)
df103.head()

# #convert already, no need to do this action
# def convert_entrez_to_symbol(entrez_file, mapping_file, output_file):
#     # Load the input files
#     df_data = pd.read_csv(entrez_file,dtype=object)
#     df_mapping = pd.read_csv(mapping_file,dtype=object)

#     # Merge the data with the mapping on the Entrez ID to get the Gene Symbol
#     df_merged = df_data.merge(df_mapping, left_on='index', right_on='NCBI GeneID', how='left')

#     # Drop the 'index' and 'NCBI GeneID' columns
#     df_merged = df_merged.drop(columns=['index', 'NCBI GeneID'])

#     # Set the 'Symbol' column as the new index and save to output file
#     df_merged.set_index('Symbol', inplace=True)
#     df_merged.to_csv(output_file)

# # File paths
# entrez_file_path = '/content/drive/MyDrive/Colab Notebooks/GSE152641_Inflammatix_COVID19_counts_entrez_for_raw counts_for Python ML.csv'
# mapping_file_path = '/content/drive/MyDrive/Colab Notebooks/gene names.csv'
# output_file_path = '/content/drive/MyDrive/Colab Notebooks/GSE152641_Inflammatix_COVID19_counts_gene symbol_for_raw counts_for Python ML.csv'

# # Call the function
# convert_entrez_to_symbol(entrez_file_path, mapping_file_path, output_file_path)

# output_file_path

df641 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/GSE152641_Inflammatix_COVID19_counts_gene symbol_for_raw counts_for Python ML.csv',dtype=object)
df641.head()

# Transpose 151
data151 = df151.values
index151 = list(df151.keys())
data151 = list(map(list, zip(*data151)))
data151 = pd.DataFrame(data151, index=index151)
data151.to_csv('TML151.csv', header=0)

# Transpose 103
data103 = df103.values
index103 = list(df103.keys())
data103 = list(map(list, zip(*data103)))
data103 = pd.DataFrame(data103, index=index103)
data103.to_csv('TML103.csv', header=0)

# Transpose 641
data641 = df641.values
index641 = list(df641.keys())
data641 = list(map(list, zip(*data641)))
data641 = pd.DataFrame(data641, index=index641)
data641.to_csv('TML641.csv', header=0)

df151 = pd.read_csv('TML151.csv',dtype=object)
df151.head()

df103 = pd.read_csv('TML103.csv',dtype=object)
df103.head()

df641 = pd.read_csv('TML641.csv',dtype=object)
df641.head()

df151 = df151.drop(df151.columns[0], axis=1)
df151.head()

df103 = df103.drop(df103.columns[0], axis=1)
df103.head()

df641 = df641.drop(df641.columns[0], axis=1)
df641.head()

df151['target'] = df151.target
df151.target.value_counts()

df103['target'] = df103.target
df103.target.value_counts()

df641['target'] = df641.target
df641.target.value_counts()

df_removetarget151 = df151.iloc[:, :-1]
df_removetarget151.head()

df_removetarget103 = df103.iloc[:, :-1]
df_removetarget103.head()

df_removetarget641 = df641.iloc[:, :-1]
df_removetarget641.head()

# select column for Sig 40 in 151
df_removetarget151 = df151.iloc[:, :-1]
df_removetarget151 = df_removetarget151[['EPSTI1',
'LINC00324',
'IFI6',
'IFI44',
'FTH1P3',
'IFIT1',
'EIF2AK2',
'DDX60',
'IFITM1',
'IRAK1',
'SP100',
'RSAD2',
'SAMD9',
'LOC283214',
'DSC3',
'ATP2A2',
'MAP2K7',
'CHD1',
'BST2',
'RNF125',
'SNORD119',
'ESR1',
'IFI27',
'NT5C3A',
'LRRC8D',
'SNORA12',
'TTC30A',
'LETM1',
'CAPN12',
'MOV10L1',
'FBXO31',
'C15orf61',
'XAF1',
'YPEL1',
'MFGE8',
'SAMD9L',
'PARP14',
'MALAT1',
'TRUB2',
'AP3B1']]
print(df_removetarget151)
df_removetarget151.head()

# select column for Sig 40 in 103
df_removetarget103 = df103.iloc[:, :-1]
df_removetarget103 = df_removetarget103[['CDC6',
'PBK',
'DTL',
'DEPDC1B',
'MELK',
'MCM10',
'TOP2A',
'RRM2',
'GINS1',
'MCM6',
'BUB1',
'CLSPN',
'CDC45',
'ESCO2',
'CDC25A',
'ASPM',
'MKI67',
'STIL',
'TYMS',
'BUB1B',
'CDCA2',
'CCNA2',
'NCAPG',
'TRIP13',
'NUSAP1',
'CDK1',
'MCM4',
'KIF15',
'EXO1',
'ZWINT',
'CHEK1',
'KIF11',
'GTSE1',
'DSCC1',
'E2F7',
'HJURP',
'PLK4',
'GLDC',
'NDC80',
'ORC1']]
print(df_removetarget103)
df_removetarget103.head()

# select column for Sig 40 in 641
df_removetarget641 = df641.iloc[:, :-1]
df_removetarget641 = df_removetarget641[['POLQ',
'CDC6',
'CLSPN',
'EPSTI1',
'CDCA2',
'RRM2',
'MCM10',
'H2BC6',
'IFI27',
'DHCR24',
'MS4A4A',
'H2AC4',
'DTL',
'TTC9',
'KIFC1',
'P2RY10',
'APOBEC3A',
'NT5E',
'CC2D2A',
'TIMM10',
'TOP1MT',
'GTSE1',
'SLC26A8',
'AMIGO1',
'GPR141',
'H2BC3',
'H3C3',
'INSC',
'CARD17',
'TWIST2',
'H2BC9',
'H3C8',
'TMEM144',
'TRIP13',
'AK4',
'FCER1A',
'HPD',
'METTL7B',
'H3C12',
'H3C7']]
print(df_removetarget641)
df_removetarget641.head()

X151 = df_removetarget151
y151 = df151.iloc[:, -1]
print(X151)
print(y151)

X103 = df_removetarget103
y103 = df103.iloc[:, -1]
print(X103)
print(y103)

X641 = df_removetarget641
y641 = df641.iloc[:, -1]
print(X641)
print(y641)

X_number151 = X151.values
y_index151 = y151.values

# standardize the dataset
scaler151 = MinMaxScaler()
X_number151 = scaler151.fit_transform(X_number151)

X_number103 = X103.values
y_index103 = y103.values

# standardize the dataset
scaler103 = MinMaxScaler()
X_number103 = scaler103.fit_transform(X_number103)

X_number641 = X641.values
y_index641 = y641.values

# standardize the dataset
scaler641 = MinMaxScaler()
X_number641 = scaler641.fit_transform(X_number641)

plt.figure( figsize=(10,5) )
y_train_series151 = pd.Series(y_index151)
y_train_series151.value_counts().plot( kind='pie', colors=['lightcoral','skyblue'], autopct='%1.2f%%' )
plt.title( 'Pos/Neg' )
plt.ylabel( '' )
plt.show()

plt.figure( figsize=(10,5) )
y_train_series103 = pd.Series(y_index103)
y_train_series103.value_counts().plot( kind='pie', colors=['lightcoral','skyblue'], autopct='%1.2f%%' )
plt.title( 'Pos/Neg' )
plt.ylabel( '' )
plt.show()

plt.figure( figsize=(10,5) )
y_train_series641 = pd.Series(y_index641)
y_train_series641.value_counts().plot( kind='pie', colors=['lightcoral','skyblue'], autopct='%1.2f%%' )
plt.title( 'Pos/Neg' )
plt.ylabel( '' )
plt.show()

print(y_index151)

print(y_index103)

print(y_index641)

# Cross Validation performance and AUC of machine learning for 4 models in GSE152075, GSE163151, GSE157103, GSE152641
# Models
models = {
    "XGBoost": XGBClassifier(colsample_bytree=0.9, learning_rate=0.1, max_depth=10, n_estimators=50, random_state=42),
    "RandomForestClassifier": RandomForestClassifier(max_depth=10, min_samples_split=5, n_estimators=100, random_state=42),
    "LogisticRegression": LogisticRegression(C=50, max_iter=5000, random_state=42),
    "SVC": SVC(kernel='rbf', C=100, gamma=0.01, probability=True, random_state=42)
}

# Datasets
datasets = {
    "GSE152075": (X_number, y_index.astype(int)),
    "GSE163151": (X_number151, y_index151.astype(int)),
    "GSE157103": (X_number103, y_index103.astype(int)),
    "GSE152641": (X_number641, y_index641.astype(int))
}

def confusion_metrics_and_report(conf_matrix):
    # Extract metrics from the confusion matrix
    TP = conf_matrix[1][1]
    TN = conf_matrix[0][0]
    FP = conf_matrix[0][1]
    FN = conf_matrix[1][0]

    print('True Positives:', TP)
    print('True Negatives:', TN)
    print('False Positives:', FP)
    print('False Negatives:', FN)

    # Calculate metrics
    accuracy = (TP + TN) / (TP + TN + FP + FN)
    mis_classification = 1 - accuracy
    sensitivity = TP / (TP + FN)
    specificity = TN / (TN + FP)
    precision = TP / (TP + FP)
    f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)

    conf_accuracy = (float(TP+TN) / float(TP + TN + FP + FN))
    conf_misclassification = 1 - conf_accuracy
    conf_sensitivity = (TP / float(TP + FN))
    conf_specificity = (TN / float(TN + FP))
    conf_precision = (TP / float(TP + FP))
    conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))

   # Print metrics
    print('-'*50)
    print(f'Accuracy: {round(conf_accuracy,3)}')
    print(f'Mis-Classification: {round(conf_misclassification,3)}')
    print(f'Sensitivity: {round(conf_sensitivity,3)}')
    print(f'Specificity: {round(conf_specificity,3)}')
    print(f'Precision: {round(conf_precision,3)}')
    print(f'f_1 Score: {round(conf_f1,3)}')

    # plot confusion_metrics
    cm_df = pd.DataFrame(conf_matrix, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])
    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g')
    plt.title('Confusion Matrix')
    plt.show()

def evaluate_model(name, model, X, y, mean_fpr, tprs, aucs):
    print(f"Evaluating model: {name}")
    print('-'*50)

    smote = SMOTE(random_state=42)
    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

    all_preds = np.zeros(y.shape)

    for train_index, test_index in skf.split(X, y):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # Oversample the training data
        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

        model_clone = clone(model)
        model_clone.fit(X_train_resampled, y_train_resampled)

        # Store predictions for computing the confusion matrix later
        all_preds[test_index] = model_clone.predict(X_test)

        # Compute ROC curve and AUC
        probas_ = model_clone.predict_proba(X_test)
        fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])

        tprs.append(np.interp(mean_fpr, fpr, tpr))
        tprs[-1][0] = 0.0
        roc_auc = auc(fpr, tpr)
        aucs.append(roc_auc)

    # Compute the confusion matrix and related metrics
    conf_matrix = confusion_matrix(y, all_preds)
    confusion_metrics_and_report(conf_matrix)
    print("\n\n")

for dataset_name, (X, y) in datasets.items():
    print(f"\nEvaluating for dataset: {dataset_name}")
    print('-' * 80)

    mean_fpr = np.linspace(0, 1, 100)
    tprs = []
    aucs = []

    for name, model in models.items():
        evaluate_model(name, model, X, y, mean_fpr, tprs, aucs)

    # Plot all the ROC Curves on a single plot for the current dataset
    plt.figure(figsize=(10, 8))
    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=0.8)
    for i, name in enumerate(models.keys()):
        mean_tpr = np.mean(tprs[i::len(models)], axis=0)
        mean_tpr[-1] = 1.0
        mean_auc = auc(mean_fpr, mean_tpr)
        plt.plot(mean_fpr, mean_tpr, label=f'{name} (AUC = {mean_auc:.3f})')

    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'Receiver Operating Characteristic (ROC) Curve for All Models ({dataset_name})')
    plt.legend(loc='lower right')
    plt.show()

    # # Train each model and save it for the current dataset
    # for model_name, model in models.items():
    #     model.fit(X, y)
    #     # Save the trained model
    #     joblib.dump(model, f"{model_name}_{dataset_name}.pkl")

# SMOTE seperately in test sets - Learning curve for 4 models results in GSE152075, GSE163151, GSE157103, GSE152641
# Models
models = {
    "XGBoost": XGBClassifier(colsample_bytree=0.9, learning_rate=0.1, max_depth=10, n_estimators=50, random_state=42),
    "RandomForestClassifier": RandomForestClassifier(max_depth=10, min_samples_split=5, n_estimators=100, random_state=42),
    "LogisticRegression": LogisticRegression(C=50, max_iter=5000, random_state=42),
    "SVC": SVC(kernel='rbf', C=100, gamma=0.01, probability=True, random_state=42)
}

# Ensure all target variables are of type int
y_index = y_index.astype(int)
y_index151 = y_index151.astype(int)
y_index103 = y_index103.astype(int)
y_index641 = y_index641.astype(int)

# Datasets
datasets = {
    "GSE152075": (X_number, y_index),
    "GSE163151": (X_number151, y_index151),
    "GSE157103": (X_number103, y_index103),
    "GSE152641": (X_number641, y_index641)
}

def plot_learning_curve_with_smote(estimator, title, X, y, cv=None, scoring=None):
    fig, axes = plt.subplots(1, 1, figsize=(5, 5))

    axes.set_title(title)
    axes.set_xlabel("Training examples")
    axes.set_ylabel(scoring)

    train_sizes = np.linspace(.1, 1.0, 5)
    smote = SMOTE(random_state=42)

    all_train_scores = []
    all_test_scores = []

    for frac in train_sizes:
        current_train_scores = []
        current_test_scores = []
        for train_index, test_index in cv.split(X, y):
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]

            # Resample only a fraction of the training data
            subset_idx = np.random.choice(len(X_train), int(frac * len(X_train)), replace=False)
            X_train_subset = X_train[subset_idx]
            y_train_subset = y_train[subset_idx]

            # Apply SMOTE
            X_train_resampled, y_train_resampled = smote.fit_resample(X_train_subset, y_train_subset)

            estimator_clone = clone(estimator)
            estimator_clone.fit(X_train_resampled, y_train_resampled)

            current_train_scores.append(accuracy_score(y_train_resampled, estimator_clone.predict(X_train_resampled)))
            current_test_scores.append(accuracy_score(y_test, estimator_clone.predict(X_test)))

        all_train_scores.append(current_train_scores)
        all_test_scores.append(current_test_scores)

    train_scores_mean = np.mean(all_train_scores, axis=1)
    train_scores_std = np.std(all_train_scores, axis=1)
    test_scores_mean = np.mean(all_test_scores, axis=1)
    test_scores_std = np.std(all_test_scores, axis=1)

    # Plotting the learning curves
    axes.grid()
    axes.fill_between(train_sizes*len(X), train_scores_mean - train_scores_std,
                      train_scores_mean + train_scores_std, alpha=0.1, color="r")
    axes.fill_between(train_sizes*len(X), test_scores_mean - test_scores_std,
                      test_scores_mean + test_scores_std, alpha=0.1, color="g")
    axes.plot(train_sizes*len(X), train_scores_mean, 'o-', color="r", label="Training score")
    axes.plot(train_sizes*len(X), test_scores_mean, 'o-', color="g", label="Cross-validation score")
    axes.legend(loc="best")
    plt.tight_layout()

    return plt



# X, y = shuffle(X_number, y_index, random_state=42)
# smote = SMOTE(random_state=42)
# X_resampled, y_resampled = smote.fit_resample(X, y)

# for name, model in models.items():
#     title = f"Learning Curves with SMOTE ({name})"
#     plot_learning_curve_with_smote(model, title, X_resampled, y_resampled, cv=StratifiedKFold(10), scoring="accuracy")
#     plt.show()



for dataset_name, (X, y) in datasets.items():
    print(f"\nEvaluating for dataset: {dataset_name}")
    print('-' * 80)

    X, y = shuffle(X, y, random_state=42)
    smote = SMOTE(random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X, y)

    for name, model in models.items():
        title = f"Learning Curves with SMOTE for {dataset_name} ({name})"
        try:
            plot_learning_curve_with_smote(model, title, X_resampled, y_resampled, cv=StratifiedKFold(10), scoring="accuracy")
            plt.show()
        except Exception as e:
            print(f"Failed to generate Learning Curves for {dataset_name} ({name}). Error: {e}")
            continue

# SMOTE seperately in test sets - check over or under fitting by cross validation with test sets in GSE152075, GSE163151, GSE157103, GSE152641

# Define the models
models = {
    "XGBoost": XGBClassifier(colsample_bytree=0.9, learning_rate=0.1, max_depth=10, n_estimators=50, random_state=42),
    "RandomForestClassifier": RandomForestClassifier(max_depth=10, min_samples_split=5, n_estimators=100, random_state=42),
    "LogisticRegression": LogisticRegression(C=50, max_iter=5000, random_state=42),
    "SVC": SVC(kernel='rbf', C=100, gamma=0.01, probability=True, random_state=42)
}

# Data sets
datasets = {
    'GSE152075': (X_number, y_index),
    'GSE163151': (X_number151, y_index151),
    'GSE157103': (X_number103, y_index103),
    'GSE152641': (X_number641, y_index641)
}

def check_over_under_fitting_with_smote_and_testset(dataset_name, model_name, model, X, y):
    print(f"Evaluating {model_name} on {dataset_name}")
    print('-'*50)

    # Splitting the data into training/validation and test sets
    X_train_val, X_test_final, y_train_val, y_test_final = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    train_scores = []
    val_scores = []
    smote = SMOTE(random_state=42)

    for train_index, test_index in skf.split(X_train_val, y_train_val):
        X_train, X_val = X_train_val[train_index], X_train_val[test_index]
        y_train, y_val = y_train_val[train_index], y_train_val[test_index]

        # Apply SMOTE only on the training data
        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

        model_clone = clone(model)
        model_clone.fit(X_train_resampled, y_train_resampled)

        train_scores.append(model_clone.score(X_train_resampled, y_train_resampled))
        val_scores.append(model_clone.score(X_val, y_val))

    mean_train_score = np.mean(train_scores)
    mean_val_score = np.mean(val_scores)

    print(f"Mean Training Score: {mean_train_score:.3f}")
    print(f"Mean Validation Score: {mean_val_score:.3f}")

    # Check the performance on the final test set
    test_score = model_clone.score(X_test_final, y_test_final)
    print(f"Final Test Score: {test_score:.3f}")

    # Check for overfitting or underfitting
    if mean_train_score > mean_val_score + 0.1:
        print("Model might be overfitting.")
    elif mean_train_score < 0.7:
        print("Model might be underfitting.")
    else:
        print("Model seems to be fitting appropriately.")
    print("\n\n")

for dataset_name, (X, y) in datasets.items():
    for model_name, model in models.items():
        check_over_under_fitting_with_smote_and_testset(dataset_name, model_name, model, X, y)

# # SMOTE seperately in test sets - check over or under fitting by train_test_split with test set in GSE152075, GSE163151, GSE157103, GSE152641
# # Define the models
# models = {
#     "XGBoost": XGBClassifier(colsample_bytree=0.9, learning_rate=0.1, max_depth=10, n_estimators=50, random_state=42),
#     "RandomForestClassifier": RandomForestClassifier(max_depth=10, min_samples_split=5, n_estimators=100, random_state=42),
#     "LogisticRegression": LogisticRegression(C=50, max_iter=5000, random_state=42),
#     "SVC": SVC(kernel='rbf', C=100, gamma=0.01, probability=True, random_state=42)
# }

# # Define datasets
# datasets = {
#     'GSE152075': (X_number, y_index),
#     'GSE163151': (X_number151, y_index151),
#     'GSE157103': (X_number103, y_index103),
#     'GSE152641': (X_number641, y_index641)
# }

# def check_overfitting_with_train_test_split(dataset_name, model_name, model, X, y):
#     print(f"Evaluating {model_name} on {dataset_name}")
#     print('-'*50)

#     # Split the data into training, validation, and test sets. devided by 70-15-15
#     X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)
#     X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp)

#     # Apply SMOTE on the training data
#     smote = SMOTE(random_state=42)
#     X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

#     # Fit the model
#     model.fit(X_train_resampled, y_train_resampled)

#     # Get the scores for training, validation, and test sets
#     train_score = accuracy_score(y_train_resampled, model.predict(X_train_resampled))
#     val_score = accuracy_score(y_val, model.predict(X_val))
#     test_score = accuracy_score(y_test, model.predict(X_test))

#     print(f"Training Accuracy: {train_score:.3f}")
#     print(f"Validation Accuracy: {val_score:.3f}")
#     print(f"Test Accuracy: {test_score:.3f}")

#     # Check for overfitting or underfitting
#     if train_score > val_score + 0.1:
#         print(f"{model_name} might be overfitting.")
#     elif train_score < 0.7:
#         print(f"{model_name} might be underfitting.")
#     else:
#         print(f"{model_name} seems to be fitting appropriately.")
#     print("-"*50)


# for dataset_name, (X, y) in datasets.items():
#     for model_name, model in models.items():
#         check_overfitting_with_train_test_split(dataset_name, model_name, clone(model), X, y)