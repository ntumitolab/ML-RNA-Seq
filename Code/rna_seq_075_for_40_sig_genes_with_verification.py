# -*- coding: utf-8 -*-
"""RNA Seq 075 for 40 sig genes with verification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zCBw68jqkA_Av-2CnvkmpVagoEmeknl4
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install shap
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn.metrics as metrics
import xgboost
import shap
import joblib

from numpy import mean, std
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, make_scorer, roc_curve, auc
from sklearn.feature_selection import SelectKBest, chi2, RFE
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold,cross_val_predict
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import TomekLinks
from sklearn.pipeline import Pipeline, make_pipeline

# %matplotlib inline

! python --version

print("shap version:", shap.__version__)

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/GSE152075_raw_counts_GEO _correct to CSV_for_COVID19 for Python ML.csv',dtype=object)
df.head()

# Transpose
data = df.values
index1 = list(df.keys())
data = list(map(list, zip(*data)))
data = pd.DataFrame(data, index=index1)
data.to_csv('TML.csv', header=0)

df = pd.read_csv('TML.csv',dtype=object)
df.head()

df = df.drop(df.columns[0], axis=1)
df.head()

df['target'] = df.target
df.target.value_counts()

# Top 40 significantly expressed genes in GSE152075
df_removetarget = df.iloc[:, :-1]
df_removetarget = df_removetarget[['IFI44L', 'XAF1', 'IFIT1', 'OAS3', 'OAS2', 'IFIT3', 'IFIT2', 'RSAD2', 'IGFBP2', 'DDX58',\
         'GBP1', 'TRIM22', 'EPSTI1', 'MX2', 'CD163', 'CMPK2', 'HERC6', 'SAMD9', 'CXCL10', 'GBP4',\
         'CRIP1', 'PARP9', 'RPLP1', 'DDX60', 'IFI44', 'IFIT5', 'RPS21', 'RPS8', 'FPR3', 'PCSK5',\
         'SAMD9L', 'DDX60L', 'OASL', 'RPL13A', 'CD300E', 'PLA2G7', 'ZEB2', 'SBK1', 'PRDX5', 'RRAD']]
print(df_removetarget)
df_removetarget.head()
#df_removetarget.to_csv('sig without target.csv')

X = df_removetarget
y = df.iloc[:, -1]
print(X)
print(y)

X = df_removetarget
y = df.iloc[:, -1]
X_number = X.values
y_index = y.values

# standardize the dataset
scaler = MinMaxScaler()
X_number = scaler.fit_transform(X_number)

print(X_number.shape)
print(y_index.shape)

X_train, X_test, y_train, y_test = train_test_split(X_number, y_index, test_size=0.2, random_state=42)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

# to avoid imbalance data - OverSampling SMOTE
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)
X_test, y_test = smote.fit_resample(X_test, y_test)

X_number, y_index = smote.fit_resample(X_number, y_index)

# to avoid imbalance data - OverSampling Tomek Link
#X_number, y_index = TomekLinks().fit_resample(X_number, y_index)

# convert to int64
y_train = y_train.astype(int)
y_test = y_test.astype(int)

print(X_number)
print(y_index)
print(X_number.shape)
print(y_index.shape)
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

plt.figure( figsize=(10,5) )
y_train_series = pd.Series(y_index)
y_train_series.value_counts().plot( kind='pie', colors=['lightcoral','skyblue'], autopct='%1.2f%%' )
plt.title( 'Pos/Neg' )
plt.ylabel( '' )
plt.show()

# Cross Validation of mschine learning for 4 models results
models = {
    "XGBoost": XGBClassifier(colsample_bytree=0.9, learning_rate=0.1, max_depth=10, n_estimators=50, random_state=17),
    "RandomForestClassifier": RandomForestClassifier(max_depth=10, min_samples_split=5, n_estimators=100, random_state=1),
    "LogisticRegression": LogisticRegression(C=50, max_iter=5000),
    "SVC": SVC(kernel='rbf', C=100, gamma=0.01, probability=True)
}

y_index = y_index.astype(int)  # Ensure y_index is of type int

def confusion_metrics_and_report(conf_matrix):
    # Extract metrics from the confusion matrix
    TP = conf_matrix[1][1]
    TN = conf_matrix[0][0]
    FP = conf_matrix[0][1]
    FN = conf_matrix[1][0]

    print('True Positives:', TP)
    print('True Negatives:', TN)
    print('False Positives:', FP)
    print('False Negatives:', FN)

    # Calculate metrics
    accuracy = (TP + TN) / (TP + TN + FP + FN)
    mis_classification = 1 - accuracy
    sensitivity = TP / (TP + FN)
    specificity = TN / (TN + FP)
    precision = TP / (TP + FP)
    f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)

    conf_accuracy = (float(TP+TN) / float(TP + TN + FP + FN))
    conf_misclassification = 1 - conf_accuracy
    conf_sensitivity = (TP / float(TP + FN))
    conf_specificity = (TN / float(TN + FP))
    conf_precision = (TP / float(TP + FP))
    conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))

   # Print metrics
    print('-'*50)
    print(f'Accuracy: {round(conf_accuracy,3)}')
    print(f'Mis-Classification: {round(conf_misclassification,3)}')
    print(f'Sensitivity: {round(conf_sensitivity,3)}')
    print(f'Specificity: {round(conf_specificity,3)}')
    print(f'Precision: {round(conf_precision,3)}')
    print(f'f_1 Score: {round(conf_f1,3)}')

    # plot confusion_metrics
    cm_df = pd.DataFrame(conf_matrix, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])
    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g')
    plt.title('Confusion Matrix')
    plt.show()

def evaluate_model(name, model, X, y):
    print(f"Evaluating model: {name}")
    print('-'*50)

    # Step 1: Perform cross-validation and get accuracy
    cv_scores = cross_val_score(model, X, y, cv=10, scoring='accuracy')
    print("Cross-validated Accuracy: %.3f +/- %.3f" % (cv_scores.mean(), cv_scores.std()))

    # Step 2: Get predictions from cross-validation
    cv_preds = cross_val_predict(model, X, y, cv=10)

    # Step 3: Compute the confusion matrix and related metrics
    conf_matrix = confusion_matrix(y, cv_preds)
    confusion_metrics_and_report(conf_matrix)
    print("\n\n")

for name, model in models.items():
    evaluate_model(name, model, X_number, y_index)

# Train each model and save it
for model_name, model in models.items():
    model.fit(X_number, y_index)
    # Save the trained model
    joblib.dump(model, f"{model_name}.pkl")

# Cross Validation of mschine learning for 4 models ROC curves

# X_numbera = np.array(X_number)
# y_indexa = np.array(y_index)

# Setting up plot for ROC Curves
plt.figure(figsize=(12, 8))
plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=0.8)

cv = StratifiedKFold(n_splits=10)

for name, model in models.items():
    tprs = []
    aucs = []
    mean_fpr = np.linspace(0, 1, 100)

    for train, test in cv.split(X_number, y_index):
        model.fit(X_number[train], y_index[train])
        probs = model.predict_proba(X_number[test])
        predsroc = probs[:, 1]
        fpr, tpr, _ = roc_curve(y_index[test], predsroc)
        tprs.append(np.interp(mean_fpr, fpr, tpr))
        tprs[-1][0] = 0.0
        roc_auc = auc(fpr, tpr)
        aucs.append(roc_auc)

    mean_tpr = np.mean(tprs, axis=0)
    mean_tpr[-1] = 1.0
    mean_auc = np.mean(aucs)
    plt.plot(mean_fpr, mean_tpr, label='%s (AUC = %0.3f)' % (name, mean_auc), lw=2, alpha=0.8)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves of Models')
plt.legend(loc="lower right")
plt.show()

df151 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/GSE163151 for COVID and Donor without virus for Python ML.csv',dtype=object)
df151.head()

df103 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/GSE157103_genes.ec_raw counts_for Python ML.csv',dtype=object)
df103.head()

def convert_entrez_to_symbol(entrez_file, mapping_file, output_file):
    # Load the input files
    df_data = pd.read_csv(entrez_file,dtype=object)
    df_mapping = pd.read_csv(mapping_file,dtype=object)

    # Merge the data with the mapping on the Entrez ID to get the Gene Symbol
    df_merged = df_data.merge(df_mapping, left_on='index', right_on='NCBI GeneID', how='left')

    # Drop the 'index' and 'NCBI GeneID' columns
    df_merged = df_merged.drop(columns=['index', 'NCBI GeneID'])

    # Set the 'Symbol' column as the new index and save to output file
    df_merged.set_index('Symbol', inplace=True)
    df_merged.to_csv(output_file)

# File paths
entrez_file_path = '/content/drive/MyDrive/Colab Notebooks/GSE152641_Inflammatix_COVID19_counts_entrez_for_raw counts_for Python ML.csv'
mapping_file_path = '/content/drive/MyDrive/Colab Notebooks/gene names.csv'
output_file_path = '/content/drive/MyDrive/Colab Notebooks/GSE152641_Inflammatix_COVID19_counts_gene symbol_for_raw counts_for Python ML.csv'

# Call the function
convert_entrez_to_symbol(entrez_file_path, mapping_file_path, output_file_path)

output_file_path

df641 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/GSE152641_Inflammatix_COVID19_counts_gene symbol_for_raw counts_for Python ML.csv',dtype=object)
df641.head()

# Transpose 151
data151 = df151.values
index151 = list(df151.keys())
data151 = list(map(list, zip(*data151)))
data151 = pd.DataFrame(data151, index=index151)
data151.to_csv('TML151.csv', header=0)

# Transpose 103
data103 = df103.values
index103 = list(df103.keys())
data103 = list(map(list, zip(*data103)))
data103 = pd.DataFrame(data103, index=index103)
data103.to_csv('TML103.csv', header=0)

# Transpose 641
data641 = df641.values
index641 = list(df641.keys())
data641 = list(map(list, zip(*data641)))
data641 = pd.DataFrame(data641, index=index641)
data641.to_csv('TML641.csv', header=0)

df151 = pd.read_csv('TML151.csv',dtype=object)
df151.head()

df103 = pd.read_csv('TML103.csv',dtype=object)
df103.head()

df641 = pd.read_csv('TML641.csv',dtype=object)
df641.head()

df151 = df151.drop(df151.columns[0], axis=1)
df151.head()

df103 = df103.drop(df103.columns[0], axis=1)
df103.head()

df641 = df641.drop(df641.columns[0], axis=1)
df641.head()

df151['target'] = df151.target
df151.target.value_counts()

df103['target'] = df103.target
df103.target.value_counts()

df641['target'] = df641.target
df641.target.value_counts()

df_removetarget151 = df151.iloc[:, :-1]
df_removetarget151.head()

df_removetarget103 = df103.iloc[:, :-1]
df_removetarget103.head()

df_removetarget641 = df641.iloc[:, :-1]
df_removetarget641.head()

# select column for Sig 40 in 151
df_removetarget151 = df151.iloc[:, :-1]
df_removetarget151 = df_removetarget151[['EPSTI1',
'LINC00324',
'IFI6',
'IFI44',
'FTH1P3',
'IFIT1',
'EIF2AK2',
'DDX60',
'IFITM1',
'IRAK1',
'SP100',
'RSAD2',
'SAMD9',
'LOC283214',
'DSC3',
'ATP2A2',
'MAP2K7',
'CHD1',
'BST2',
'RNF125',
'SNORD119',
'ESR1',
'IFI27',
'NT5C3A',
'LRRC8D',
'SNORA12',
'TTC30A',
'LETM1',
'CAPN12',
'MOV10L1',
'FBXO31',
'C15orf61',
'XAF1',
'YPEL1',
'MFGE8',
'SAMD9L',
'PARP14',
'MALAT1',
'TRUB2',
'AP3B1']]
print(df_removetarget151)
df_removetarget151.head()

# select column for Sig 40 with 075 for 151
df_removetarget151 = df151.iloc[:, :-1]
df_removetarget151 = df_removetarget151[['IFI44L', 'XAF1', 'IFIT1', 'OAS3', 'OAS2', 'IFIT3', 'IFIT2', 'RSAD2', 'IGFBP2', 'DDX58',\
         'GBP1', 'TRIM22', 'EPSTI1', 'MX2', 'CD163', 'CMPK2', 'HERC6', 'SAMD9', 'CXCL10', 'GBP4',\
         'CRIP1', 'PARP9', 'RPLP1', 'DDX60', 'IFI44', 'IFIT5', 'RPS21', 'RPS8', 'FPR3', 'PCSK5',\
         'SAMD9L', 'DDX60L', 'OASL', 'RPL13A', 'CD300E', 'PLA2G7', 'ZEB2', 'SBK1', 'PRDX5', 'RRAD']]
print(df_removetarget151)
df_removetarget151.head()

# select column for Sig 40 in 103
df_removetarget103 = df103.iloc[:, :-1]
df_removetarget103 = df_removetarget103[['CDC6',
'PBK',
'DTL',
'DEPDC1B',
'MELK',
'MCM10',
'TOP2A',
'RRM2',
'GINS1',
'MCM6',
'BUB1',
'CLSPN',
'CDC45',
'ESCO2',
'CDC25A',
'ASPM',
'MKI67',
'STIL',
'TYMS',
'BUB1B',
'CDCA2',
'CCNA2',
'NCAPG',
'TRIP13',
'NUSAP1',
'CDK1',
'MCM4',
'KIF15',
'EXO1',
'ZWINT',
'CHEK1',
'KIF11',
'GTSE1',
'DSCC1',
'E2F7',
'HJURP',
'PLK4',
'GLDC',
'NDC80',
'ORC1']]
print(df_removetarget103)
df_removetarget103.head()

# select column for Sig 40 with 075 for 103
df_removetarget103 = df103.iloc[:, :-1]
df_removetarget103 = df_removetarget103[['IFI44L', 'XAF1', 'IFIT1', 'OAS3', 'OAS2', 'IFIT3', 'IFIT2', 'RSAD2', 'IGFBP2', 'DDX58',\
         'GBP1', 'TRIM22', 'EPSTI1', 'MX2', 'CD163', 'CMPK2', 'HERC6', 'SAMD9', 'CXCL10', 'GBP4',\
         'CRIP1', 'PARP9', 'RPLP1', 'DDX60', 'IFI44', 'IFIT5', 'RPS21', 'RPS8', 'FPR3', 'PCSK5',\
         'SAMD9L', 'DDX60L', 'OASL', 'RPL13A', 'CD300E', 'PLA2G7', 'ZEB2', 'SBK1', 'PRDX5', 'RRAD']]
print(df_removetarget103)
df_removetarget103.head()

# select column for Sig 40 in 641
df_removetarget641 = df641.iloc[:, :-1]
df_removetarget641 = df_removetarget641[['POLQ',
'CDC6',
'CLSPN',
'EPSTI1',
'CDCA2',
'RRM2',
'MCM10',
'H2BC6',
'IFI27',
'DHCR24',
'MS4A4A',
'H2AC4',
'DTL',
'TTC9',
'KIFC1',
'P2RY10',
'APOBEC3A',
'NT5E',
'CC2D2A',
'TIMM10',
'TOP1MT',
'GTSE1',
'SLC26A8',
'AMIGO1',
'GPR141',
'H2BC3',
'H3C3',
'INSC',
'CARD17',
'TWIST2',
'H2BC9',
'H3C8',
'TMEM144',
'TRIP13',
'AK4',
'FCER1A',
'HPD',
'METTL7B',
'H3C12',
'H3C7']]
print(df_removetarget641)
df_removetarget641.head()

# check missing columns 641
selected_columns = ['IFI44L', 'XAF1', 'IFIT1', 'OAS3', 'OAS2', 'IFIT3', 'IFIT2', 'RSAD2', 'IGFBP2', 'DDX58',\
                    'GBP1', 'TRIM22', 'EPSTI1', 'MX2', 'CD163', 'CMPK2', 'HERC6', 'SAMD9', 'CXCL10', 'GBP4',\
                    'CRIP1', 'PARP9', 'RPLP1', 'DDX60', 'IFI44', 'IFIT5', 'RPS21', 'RPS8', 'FPR3', 'PCSK5',\
                    'SAMD9L', 'DDX60L', 'OASL', 'RPL13A', 'CD300E', 'PLA2G7', 'ZEB2', 'SBK1', 'PRDX5', 'RRAD']

missing_columns = [col for col in selected_columns if col not in df_removetarget641.columns]

print("Missing columns:", missing_columns)

# select column for Sig 40 with 075 for 641
df_removetarget641 = df641.iloc[:, :-1]
df_removetarget641 = df_removetarget641[['IFI44L', 'XAF1', 'IFIT1', 'OAS3', 'OAS2', 'IFIT3', 'IFIT2', 'RSAD2', 'IGFBP2', 'DDX58',\
         'GBP1', 'TRIM22', 'EPSTI1', 'MX2', 'CD163', 'CMPK2', 'HERC6', 'SAMD9', 'CXCL10', 'GBP4',\
         'CRIP1', 'PARP9', 'RPLP1', 'DDX60', 'IFI44', 'IFIT5', 'RPS21', 'RPS8', 'FPR3', 'PCSK5',\
         'SAMD9L', 'DDX60L', 'OASL', 'RPL13A', 'CD300E', 'PLA2G7', 'ZEB2', 'SBK1', 'PRDX5', 'RRAD']]
print(df_removetarget641)
df_removetarget641.head()

X151 = df_removetarget151
y151 = df151.iloc[:, -1]
print(X151)
print(y151)

X103 = df_removetarget103
y103 = df103.iloc[:, -1]
print(X103)
print(y103)

X641 = df_removetarget641
y641 = df641.iloc[:, -1]
print(X641)
print(y641)

X_number151 = X151.values
y_index151 = y151.values

# standardize the dataset
scaler151 = MinMaxScaler()
X_number151 = scaler151.fit_transform(X_number151)

X_train151, X_test151, y_train151, y_test151 = train_test_split(X_number151, y_index151, test_size=0.2, random_state=42)

# to avoid imbalance data - OverSampling SMOTE
X_train151, y_train151 = SMOTE().fit_resample(X_train151, y_train151)
X_number151, y_index151 = SMOTE().fit_resample(X_number151, y_index151)

len(X_train151), len(X_test151), len(y_train151), len(y_test151)
print(y_index151)
print(y_index151.shape)
print(y_train151.shape)
print(y_test151.shape)

X_number103 = X103.values
y_index103 = y103.values

# standardize the dataset
scaler103 = MinMaxScaler()
X_number103 = scaler103.fit_transform(X_number103)

X_train103, X_test103, y_train103, y_test103 = train_test_split(X_number103, y_index103, test_size=0.2, random_state=42)

# to avoid imbalance data - OverSampling SMOTE
X_train103, y_train103 = SMOTE().fit_resample(X_train103, y_train103)
X_number103, y_index103 = SMOTE().fit_resample(X_number103, y_index103)

len(X_train103), len(X_test103), len(y_train103), len(y_test103)
print(y_index103)
print(y_index103.shape)
print(y_train103.shape)
print(y_test103.shape)

X_number641 = X641.values
y_index641 = y641.values

# standardize the dataset
scaler641 = MinMaxScaler()
X_number641 = scaler641.fit_transform(X_number641)

X_train641, X_test641, y_train641, y_test641 = train_test_split(X_number641, y_index641, test_size=0.2, random_state=42)

# to avoid imbalance data - OverSampling SMOTE
X_train641, y_train641 = SMOTE().fit_resample(X_train641, y_train641)
X_number641, y_index641 = SMOTE().fit_resample(X_number641, y_index641)

len(X_train641), len(X_test641), len(y_train641), len(y_test641)
print(y_index641)
print(y_index641.shape)
print(y_train641.shape)
print(y_test641.shape)

plt.figure( figsize=(10,5) )
y_train_series151 = pd.Series(y_index151)
y_train_series151.value_counts().plot( kind='pie', colors=['lightcoral','skyblue'], autopct='%1.2f%%' )
plt.title( 'Pos/Neg' )  # 圖標題
plt.ylabel( '' )
plt.show()

plt.figure( figsize=(10,5) )
y_train_series103 = pd.Series(y_index103)
y_train_series103.value_counts().plot( kind='pie', colors=['lightcoral','skyblue'], autopct='%1.2f%%' )
plt.title( 'Pos/Neg' )  # 圖標題
plt.ylabel( '' )
plt.show()

plt.figure( figsize=(10,5) )
y_train_series641 = pd.Series(y_index641)
y_train_series641.value_counts().plot( kind='pie', colors=['lightcoral','skyblue'], autopct='%1.2f%%' )
plt.title( 'Pos/Neg' )  # 圖標題
plt.ylabel( '' )
plt.show()

print(y_index151)

print(y_index103)

print(y_index641)

# For 075 ML model velidation in 151
# Define the model names and their associated file paths
model_names151 = ["XGBoost", "RandomForestClassifier", "LogisticRegression", "SVC"]
model_files151 = [f"{name}.pkl" for name in model_names151]

# Load all models
loaded_models151 = {}
for name, file in zip(model_names151, model_files151):
    loaded_models151[name] = joblib.load(file)

# Assume new_data is the data you want to predict on, and it has been preprocessed
predictions151 = {}
for name, model in loaded_models151.items():
    predictions151[name] = model.predict(X_number151)

print(predictions151)

y_index151 = y_index151.astype(int)
accuracies151 = {}
for name, preds in predictions151.items():
    acc151 = accuracy_score(y_index151, preds)
    accuracies151[name] = acc151

print(accuracies151)

# For 075 ML model velidation in 103
# Define the model names and their associated file paths
model_names103 = ["XGBoost", "RandomForestClassifier", "LogisticRegression", "SVC"]
model_files103 = [f"{name}.pkl" for name in model_names103]

# Load all models
loaded_models103 = {}
for name, file in zip(model_names103, model_files103):
    loaded_models103[name] = joblib.load(file)

# Assume new_data is the data you want to predict on, and it has been preprocessed
predictions103 = {}
for name, model in loaded_models103.items():
    predictions103[name] = model.predict(X_number103)

print(predictions103)

y_index103 = y_index103.astype(int)
accuracies103 = {}
for name, preds in predictions103.items():
    acc103 = accuracy_score(y_index103, preds)
    accuracies103[name] = acc103

print(accuracies103)

# For 075 ML model velidation in 641
# Define the model names and their associated file paths
model_names641 = ["XGBoost", "RandomForestClassifier", "LogisticRegression", "SVC"]
model_files641 = [f"{name}.pkl" for name in model_names641]

# Load all models
loaded_models641 = {}
for name, file in zip(model_names641, model_files641):
    loaded_models641[name] = joblib.load(file)

# Assume new_data is the data you want to predict on, and it has been preprocessed
predictions641 = {}
for name, model in loaded_models641.items():
    predictions641[name] = model.predict(X_number641)

print(predictions641)

y_index641 = y_index641.astype(int)
accuracies641 = {}
for name, preds in predictions641.items():
    acc641 = accuracy_score(y_index641, preds)
    accuracies641[name] = acc641

print(accuracies641)

# Cross Validation of mschine learning for 4 models results for 151
models = {
    "XGBoost": XGBClassifier(colsample_bytree=0.9, learning_rate=0.1, max_depth=10, n_estimators=50, random_state=17),
    "RandomForestClassifier": RandomForestClassifier(max_depth=10, min_samples_split=5, n_estimators=100, random_state=1),
    "LogisticRegression": LogisticRegression(C=50, max_iter=5000),
    "SVC": SVC(kernel='rbf', C=100, gamma=0.01, probability=True)
}

y_index151 = y_index151.astype(int)  # Ensure y_index is of type int

def confusion_metrics_and_report(conf_matrix):
    # Extract metrics from the confusion matrix
    TP = conf_matrix[1][1]
    TN = conf_matrix[0][0]
    FP = conf_matrix[0][1]
    FN = conf_matrix[1][0]

    print('True Positives:', TP)
    print('True Negatives:', TN)
    print('False Positives:', FP)
    print('False Negatives:', FN)

    # Calculate metrics
    accuracy = (TP + TN) / (TP + TN + FP + FN)
    mis_classification = 1 - accuracy
    sensitivity = TP / (TP + FN)
    specificity = TN / (TN + FP)
    precision = TP / (TP + FP)
    f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)

    conf_accuracy = (float(TP+TN) / float(TP + TN + FP + FN))
    conf_misclassification = 1 - conf_accuracy
    conf_sensitivity = (TP / float(TP + FN))
    conf_specificity = (TN / float(TN + FP))
    conf_precision = (TP / float(TP + FP))
    conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))

   # Print metrics
    print('-'*50)
    print(f'Accuracy: {round(conf_accuracy,3)}')
    print(f'Mis-Classification: {round(conf_misclassification,3)}')
    print(f'Sensitivity: {round(conf_sensitivity,3)}')
    print(f'Specificity: {round(conf_specificity,3)}')
    print(f'Precision: {round(conf_precision,3)}')
    print(f'f_1 Score: {round(conf_f1,3)}')

    # plot confusion_metrics
    cm_df = pd.DataFrame(conf_matrix, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])
    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g')
    plt.title('Confusion Matrix')
    plt.show()

def evaluate_model(name, model, X, y):
    print(f"Evaluating model: {name}")
    print('-'*50)

    # Step 1: Perform cross-validation and get accuracy
    cv_scores = cross_val_score(model, X, y, cv=10, scoring='accuracy')
    print("Cross-validated Accuracy: %.3f +/- %.3f" % (cv_scores.mean(), cv_scores.std()))

    # Step 2: Get predictions from cross-validation
    cv_preds = cross_val_predict(model, X, y, cv=10)

    # Step 3: Compute the confusion matrix and related metrics
    conf_matrix = confusion_matrix(y, cv_preds)
    confusion_metrics_and_report(conf_matrix)
    print("\n\n")


for name, model in models.items():
    evaluate_model(name, model, X_number151, y_index151)

# Cross Validation of mschine learning for 4 models ROC curves for 151

# X_numbera = np.array(X_number)
# y_indexa = np.array(y_index)

# Setting up plot for ROC Curves
plt.figure(figsize=(12, 8))
plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=0.8)

cv = StratifiedKFold(n_splits=10)

for name, model in models.items():
    tprs = []
    aucs = []
    mean_fpr = np.linspace(0, 1, 100)

    for train, test in cv.split(X_number151, y_index151):
        model.fit(X_number151[train], y_index151[train])
        probs = model.predict_proba(X_number151[test])
        predsroc = probs[:, 1]
        fpr, tpr, _ = roc_curve(y_index151[test], predsroc)
        tprs.append(np.interp(mean_fpr, fpr, tpr))
        tprs[-1][0] = 0.0
        roc_auc = auc(fpr, tpr)
        aucs.append(roc_auc)

    mean_tpr = np.mean(tprs, axis=0)
    mean_tpr[-1] = 1.0
    mean_auc = np.mean(aucs)
    plt.plot(mean_fpr, mean_tpr, label='%s (AUC = %0.3f)' % (name, mean_auc), lw=2, alpha=0.8)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves of Models')
plt.legend(loc="lower right")
plt.show()

# Cross Validation of mschine learning for 4 models results for 103
models = {
    "XGBoost": XGBClassifier(colsample_bytree=0.9, learning_rate=0.1, max_depth=10, n_estimators=50, random_state=17),
    "RandomForestClassifier": RandomForestClassifier(max_depth=10, min_samples_split=5, n_estimators=100, random_state=1),
    "LogisticRegression": LogisticRegression(C=50, max_iter=5000),
    "SVC": SVC(kernel='rbf', C=100, gamma=0.01, probability=True)
}

y_index103 = y_index103.astype(int)  # Ensure y_index is of type int

def confusion_metrics_and_report(conf_matrix):
    # Extract metrics from the confusion matrix
    TP = conf_matrix[1][1]
    TN = conf_matrix[0][0]
    FP = conf_matrix[0][1]
    FN = conf_matrix[1][0]

    print('True Positives:', TP)
    print('True Negatives:', TN)
    print('False Positives:', FP)
    print('False Negatives:', FN)

    # Calculate metrics
    accuracy = (TP + TN) / (TP + TN + FP + FN)
    mis_classification = 1 - accuracy
    sensitivity = TP / (TP + FN)
    specificity = TN / (TN + FP)
    precision = TP / (TP + FP)
    f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)

    conf_accuracy = (float(TP+TN) / float(TP + TN + FP + FN))
    conf_misclassification = 1 - conf_accuracy
    conf_sensitivity = (TP / float(TP + FN))
    conf_specificity = (TN / float(TN + FP))
    conf_precision = (TP / float(TP + FP))
    conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))

   # Print metrics
    print('-'*50)
    print(f'Accuracy: {round(conf_accuracy,3)}')
    print(f'Mis-Classification: {round(conf_misclassification,3)}')
    print(f'Sensitivity: {round(conf_sensitivity,3)}')
    print(f'Specificity: {round(conf_specificity,3)}')
    print(f'Precision: {round(conf_precision,3)}')
    print(f'f_1 Score: {round(conf_f1,3)}')

    # plot confusion_metrics
    cm_df = pd.DataFrame(conf_matrix, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])
    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g')
    plt.title('Confusion Matrix')
    plt.show()

def evaluate_model(name, model, X, y):
    print(f"Evaluating model: {name}")
    print('-'*50)

    # Step 1: Perform cross-validation and get accuracy
    cv_scores = cross_val_score(model, X, y, cv=10, scoring='accuracy')
    print("Cross-validated Accuracy: %.3f +/- %.3f" % (cv_scores.mean(), cv_scores.std()))

    # Step 2: Get predictions from cross-validation
    cv_preds = cross_val_predict(model, X, y, cv=10)

    # Step 3: Compute the confusion matrix and related metrics
    conf_matrix = confusion_matrix(y, cv_preds)
    confusion_metrics_and_report(conf_matrix)
    print("\n\n")


for name, model in models.items():
    evaluate_model(name, model, X_number103, y_index103)

# Cross Validation of mschine learning for 4 models ROC curves for 103

# X_numbera = np.array(X_number)
# y_indexa = np.array(y_index)

# Setting up plot for ROC Curves
plt.figure(figsize=(12, 8))
plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=0.8)

cv = StratifiedKFold(n_splits=10)

for name, model in models.items():
    tprs = []
    aucs = []
    mean_fpr = np.linspace(0, 1, 100)

    for train, test in cv.split(X_number103, y_index103):
        model.fit(X_number103[train], y_index103[train])
        probs = model.predict_proba(X_number103[test])
        predsroc = probs[:, 1]
        fpr, tpr, _ = roc_curve(y_index103[test], predsroc)
        tprs.append(np.interp(mean_fpr, fpr, tpr))
        tprs[-1][0] = 0.0
        roc_auc = auc(fpr, tpr)
        aucs.append(roc_auc)

    mean_tpr = np.mean(tprs, axis=0)
    mean_tpr[-1] = 1.0
    mean_auc = np.mean(aucs)
    plt.plot(mean_fpr, mean_tpr, label='%s (AUC = %0.3f)' % (name, mean_auc), lw=2, alpha=0.8)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves of Models')
plt.legend(loc="lower right")
plt.show()

# Cross Validation of mschine learning for 4 models results for 641
models = {
    "XGBoost": XGBClassifier(colsample_bytree=0.9, learning_rate=0.1, max_depth=10, n_estimators=50, random_state=17),
    "RandomForestClassifier": RandomForestClassifier(max_depth=10, min_samples_split=5, n_estimators=100, random_state=1),
    "LogisticRegression": LogisticRegression(C=50, max_iter=5000),
    "SVC": SVC(kernel='rbf', C=100, gamma=0.01, probability=True)
}

y_index641 = y_index641.astype(int)  # Ensure y_index is of type int

def confusion_metrics_and_report(conf_matrix):
    # Extract metrics from the confusion matrix
    TP = conf_matrix[1][1]
    TN = conf_matrix[0][0]
    FP = conf_matrix[0][1]
    FN = conf_matrix[1][0]

    print('True Positives:', TP)
    print('True Negatives:', TN)
    print('False Positives:', FP)
    print('False Negatives:', FN)

    # Calculate metrics
    accuracy = (TP + TN) / (TP + TN + FP + FN)
    mis_classification = 1 - accuracy
    sensitivity = TP / (TP + FN)
    specificity = TN / (TN + FP)
    precision = TP / (TP + FP)
    f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)

    conf_accuracy = (float(TP+TN) / float(TP + TN + FP + FN))
    conf_misclassification = 1 - conf_accuracy
    conf_sensitivity = (TP / float(TP + FN))
    conf_specificity = (TN / float(TN + FP))
    conf_precision = (TP / float(TP + FP))
    conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))

   # Print metrics
    print('-'*50)
    print(f'Accuracy: {round(conf_accuracy,3)}')
    print(f'Mis-Classification: {round(conf_misclassification,3)}')
    print(f'Sensitivity: {round(conf_sensitivity,3)}')
    print(f'Specificity: {round(conf_specificity,3)}')
    print(f'Precision: {round(conf_precision,3)}')
    print(f'f_1 Score: {round(conf_f1,3)}')

    # plot confusion_metrics
    cm_df = pd.DataFrame(conf_matrix, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])
    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g')
    plt.title('Confusion Matrix')
    plt.show()

def evaluate_model(name, model, X, y):
    print(f"Evaluating model: {name}")
    print('-'*50)

    # Step 1: Perform cross-validation and get accuracy
    cv_scores = cross_val_score(model, X, y, cv=10, scoring='accuracy')
    print("Cross-validated Accuracy: %.3f +/- %.3f" % (cv_scores.mean(), cv_scores.std()))

    # Step 2: Get predictions from cross-validation
    cv_preds = cross_val_predict(model, X, y, cv=10)

    # Step 3: Compute the confusion matrix and related metrics
    conf_matrix = confusion_matrix(y, cv_preds)
    confusion_metrics_and_report(conf_matrix)
    print("\n\n")


for name, model in models.items():
    evaluate_model(name, model, X_number641, y_index641)

# Cross Validation of mschine learning for 4 models ROC curves for 641

# X_numbera = np.array(X_number)
# y_indexa = np.array(y_index)

# Setting up plot for ROC Curves
plt.figure(figsize=(12, 8))
plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=0.8)

cv = StratifiedKFold(n_splits=10)

for name, model in models.items():
    tprs = []
    aucs = []
    mean_fpr = np.linspace(0, 1, 100)

    for train, test in cv.split(X_number641, y_index641):
        model.fit(X_number641[train], y_index641[train])
        probs = model.predict_proba(X_number641[test])
        predsroc = probs[:, 1]
        fpr, tpr, _ = roc_curve(y_index641[test], predsroc)
        tprs.append(np.interp(mean_fpr, fpr, tpr))
        tprs[-1][0] = 0.0
        roc_auc = auc(fpr, tpr)
        aucs.append(roc_auc)

    mean_tpr = np.mean(tprs, axis=0)
    mean_tpr[-1] = 1.0
    mean_auc = np.mean(aucs)
    plt.plot(mean_fpr, mean_tpr, label='%s (AUC = %0.3f)' % (name, mean_auc), lw=2, alpha=0.8)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves of Models')
plt.legend(loc="lower right")
plt.show()

# Train_test_split of mschine learning for 4 models results

# Convert to int for compatibility with the models
# y_train = y_train.astype(int)
# y_test = y_test.astype(int)



def evaluate_model(name, model):
    print(f"Evaluating model: {name}")
    print('-'*50)

    # Fit the model
    model.fit(X_train, y_train)

    # Predictions
    y_preds = model.predict(X_test)

    # Classification report
    print(classification_report(y_test, y_preds, digits=3))

    # Confusion matrix
    cm = confusion_matrix(y_test, y_preds)
    TP = cm[1][1]
    TN = cm[0][0]
    FP = cm[0][1]
    FN = cm[1][0]
    print('True Positives:', TP)
    print('True Negatives:', TN)
    print('False Positives:', FP)
    print('False Negatives:', FN)

    # Metrics
    accuracy = (TP + TN) / (TP + TN + FP + FN)
    mis_classification = 1 - accuracy
    sensitivity = TP / (TP + FN)
    specificity = TN / (TN + FP)
    precision = TP / (TP + FP)
    f1_score_val = 2 * (precision * sensitivity) / (precision + sensitivity)

    print('-'*50)
    print(f'Accuracy: {round(accuracy,3)}')
    print(f'Mis-Classification: {round(mis_classification,3)}')
    print(f'Sensitivity: {round(sensitivity,3)}')
    print(f'Specificity: {round(specificity,3)}')
    print(f'Precision: {round(precision,3)}')
    print(f'f_1 Score: {round(f1_score_val,3)}')

    # Train and Test Accuracy
    print('Training set accuracy:', model.score(X_train, y_train))
    print('Testing set accuracy:', model.score(X_test, y_test))

    # Plot confusion matrix
    cm_df = pd.DataFrame(cm, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])
    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g')
    plt.title('Confusion Matrix')
    plt.show()
    print('='*100)

for name, model in models.items():
    evaluate_model(name, model)

# Train_test_split of mschine learning for 4 models ROC curves
plt.figure(figsize=(10, 8))

for name, model in models.items():
    model.fit(X_train, y_train)
    probas_ = model.predict_proba(X_test)
    fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])
    auc_score = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{name} (AUC = {auc_score:.3f})")

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()